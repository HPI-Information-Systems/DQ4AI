""" Script used to visualize the contents of a raw results.json file. This file is generated by running the
``run_experiments.py`` script. For plotting of the preprocessed results.json file refer to the ``export_plots.py`` script.
This preprocessing is very specific to the JSON format expected to be produced when running experiments for clustering.

Execution and configuration via CLI parameters.
"""

import argparse
import json

from pathlib import Path


def merge_seeded_datasets(results_dict):
    """ Merges datasets that share the same prefix after asking for user confirmation. This is required for the clustering,
    as the three datasets are all pre-sampled with 5 different random seeds, but their results are supposed to be
    merged and then averaged in further processing steps.
    """
    ds_names = list(results_dict.keys())

    potential_base_datasets = list(set([dname.split('_')[0] for dname in ds_names]))
    print('Identified the following potential base dataset name stems (no suffix):')
    for i, base_name in enumerate(potential_base_datasets):
        print(f'  {i}: {base_name}')

    target_datasets = [int(i) for i in
                       input('Which ones should be used to aggregate? (provide list like 1,2,3) ').split(',')]

    target_ds_prefixes = [potential_base_datasets[i] for i in target_datasets]

    merged_dict = dict()

    for k, v in results_dict.items():
        if k.split('_')[0] in target_ds_prefixes:
            new_key = k.split('.')[0].split('_')[0] + '.' + k.split('.')[1]
            if new_key not in merged_dict:
                merged_dict[new_key] = {poll_name: {'n_seeds': 0, 'configurations': 0} for poll_name in v.keys()}
            for poll_name, res_vals in v.items():
                # special handling for Uniqueness polluter - for our evaluation we only want to look at normally
                # distributed duplicates
                if 'distribution' in list(res_vals.keys())[0]:
                    res_vals = {conf: ress for conf, ress in res_vals.items() if 'normal' in conf}
                # special handling for Consistent Representation polluter - for our evaluation we only want to consider
                # the baseline experiment (random_seed == 42) or experiments with 5 representations per categorical val
                elif 'new_representations' in list(res_vals.keys())[0]:
                    res_vals = {
                        conf: ress for conf, ress in res_vals.items()
                        if eval(conf)['random_seed'] == '42'
                           or len(list(list(eval(eval(conf)['new_representations']).values())[0].values())[0]) == 4
                    }
                merged_dict[new_key][poll_name].update(res_vals)
                merged_dict[new_key][poll_name]['n_seeds'] += 1
                merged_dict[new_key][poll_name]['configurations'] = len(res_vals.keys())
        else:
            merged_dict[k] = v

    return merged_dict


def merge_runs(results_dict):
    """ Merges whole runs from inside of the results.json file. Each time a new run is started, its results
    are saved under the start timestamp as key in the results.json. The user is asked to confirm which of these
    runs should be merged to then be further processed as if they were one run.
    """
    print('Choose which run to merge:')
    for i, k in enumerate(list(results_dict.keys())):
        print(f'  {i}: {k}')
    runs_to_merge = [int(i) for i in input('Which runs should be merged? (provide list like 1,2,3) ').split(',')]

    runs_to_merge = [list(results_dict.keys())[i] for i in runs_to_merge]

    merged_dict = {'merged_runs': dict()}

    for run_key in runs_to_merge:
        for dataset, ds_dict in results_dict[run_key].items():
            if dataset not in merged_dict['merged_runs'].keys():
                merged_dict['merged_runs'][dataset] = ds_dict
            else:
                for polluter, polluter_dict in results_dict[run_key][dataset].items():
                    if polluter not in merged_dict['merged_runs'][dataset].keys():
                        merged_dict['merged_runs'][dataset][polluter] = polluter_dict
                    else:
                        for poll_config, poll_results in results_dict[run_key][dataset][polluter].items():
                            if poll_config not in merged_dict['merged_runs'][dataset][polluter].keys():
                                merged_dict['merged_runs'][dataset][polluter][poll_config] = poll_results
                            else:
                                for algorithm, algo_results in results_dict[run_key][dataset][polluter][poll_config].items():
                                    if algorithm not in merged_dict['merged_runs'][dataset][polluter][poll_config].keys():
                                        merged_dict['merged_runs'][dataset][polluter][poll_config][algorithm] = algo_results
                                    elif algorithm != 'quality':
                                        print(f'WARNING: Algorithm {algorithm} already in merged dict for key chain:\n'
                                              f'         {run_key}>{dataset}>{polluter}>{poll_config}\n'
                                              f'Overwriting anyways!')
                                        merged_dict['merged_runs'][dataset][polluter][poll_config][algorithm] = algo_results
    return merged_dict


def main(args):
    with open(args.results, 'r') as f:
        results = json.load(f)

    if input('Do you want to merge multiple runs? (y/n)') == 'y':
        merged_dict = merge_runs(results)
        with open(args.output, 'w') as f:
            json.dump(merged_dict, f, sort_keys=True, indent=4)

        if input('Do you want to continue preprocessing the merged result? (y/n)') == 'y':
            results = merged_dict
        else:
            return

    print('Choose which run to preprocess for:')
    for i, k in enumerate(list(results.keys())):
        print(f'  {i}: {k}')
    run_to_plot = int(input('Which run to preprocess (number)?'))

    results = results[list(results.keys())[run_to_plot]]

    results = merge_seeded_datasets(results)

    with open(args.output, 'w') as f:
        json.dump(results, f, sort_keys=False, indent=4)


if __name__ == '__main__':
    parser = argparse.ArgumentParser('Script reading a results.json and plotting the metrics recorded.')

    parser.add_argument('--results', required=True, type=Path, help='Path to the results.json to plot from.')
    parser.add_argument('--output', required=True, type=Path, help='Path to write the preprocessed JSON to.')

    main(parser.parse_args())
